{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas.api.types import is_string_dtype\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo:\n",
    "\n",
    "1. <s> Figure out how to get stuff out of the JSON columns </s>\n",
    "3. <s> Prepare the dependent variable so it predicts what it needs to </s>\n",
    "4. <s> Make sure categorical variables are correctly encoded </s>\n",
    "5. <s> one-hot encode variables with cardinality <7 </s>\n",
    "6. <s> Get variables out of the date variable </s>\n",
    "7. Not sure I understand the distribution of \"totals.transactionRevenue\". Why is the mode around 16? \n",
    "7. <s> Deal with missing values </s>\n",
    "8. Preprocess the test data, too. Run the transformations on the joint df and then divide\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data hidden inside json columns out into separate columns \n",
    "# credit: julian3833 https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields\n",
    "\n",
    "def load_df(csv_path=f'{PATH}train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str',\n",
    "                            'totals.transactionRevenu': 'int64'\n",
    "                           },\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=None\n",
    "# nrows=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-7b972ee063d7>\u001b[0m in \u001b[0;36mload_df\u001b[0;34m(csv_path, nrows)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mJSON_COLUMNS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcolumn_as_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mcolumn_as_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{column}.{subcolumn}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_as_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_as_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/pandas/io/json/normalize.py\u001b[0m in \u001b[0;36mjson_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrecord_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         if any([[isinstance(x, dict)\n\u001b[0;32m--> 198\u001b[0;31m                 for x in compat.itervalues(y)] for y in data]):\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;31m# naive normalization, this is idempotent for flat records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# and potentially will inflate the data considerably for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/pandas/io/json/normalize.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrecord_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         if any([[isinstance(x, dict)\n\u001b[0;32m--> 198\u001b[0;31m                 for x in compat.itervalues(y)] for y in data]):\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;31m# naive normalization, this is idempotent for flat records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# and potentially will inflate the data considerably for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36mitervalues\u001b[0;34m(obj, **kw)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time df = load_df(nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date span\n",
    "df.date.min(), df.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add date variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date vars. Adapted from the fastai library\n",
    "\n",
    "def add_datepart(df, fldname, drop=False, time=False):\n",
    "    \n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, format='%Y%m%d')\n",
    "        \n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: \n",
    "        attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: \n",
    "        df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "        \n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    if drop: \n",
    "        df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(df, 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting string variables into category variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = []\n",
    "for n,c in df.items():\n",
    "    if is_string_dtype(c): \n",
    "        cat_vars.append(n)\n",
    "        df[n] = c.astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick look at the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_index()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['totals.transactionRevenue'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['totals.transactionRevenue'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.info(null_counts=True)\n",
    "cols_with_nulls = []\n",
    "for col in df.columns: \n",
    "    if (df[col].hasnans) & (col != \"totals.transactionRevenue\"): \n",
    "        cols_with_nulls.append(col)\n",
    "        \n",
    "cols_with_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    453023\n",
       "1      450630\n",
       "Name: totals.bounces, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['totals.bounces'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_with_nulls:\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        df[col+'_na'] = pd.isnull(df[col])\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    elif pd.api.types.is_categorical_dtype(df[col]):\n",
    "        df[col+'_na'] = pd.isnull(df[col])\n",
    "        df[col] =  df[col].cat.add_categories([\"-1\"])\n",
    "        df[col] = df[col].fillna(\"-1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_nulls = []\n",
    "for col in df.columns: \n",
    "    if df[col].hasnans: \n",
    "        cols_with_nulls.append(col)\n",
    "        \n",
    "cols_with_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    453023\n",
       "1     450630\n",
       "Name: totals.bounces, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['totals.bounces'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    453023\n",
       "1     450630\n",
       "Name: totals.bounces, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['totals.bounces'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding categorical variables with cardinality <= 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify(df, min_cardinality=0, max_cardinality=15):\n",
    "\n",
    "    to_dummify = []\n",
    "    \n",
    "    for n,c in df.items():\n",
    "        \n",
    "        if str(df.dtypes[n]) == 'category':\n",
    "            if ((len(df[n].cat.categories)) > min_cardinality) \\\n",
    "            & ((len(df[n].cat.categories)) <= max_cardinality):\n",
    "                to_dummify.append(n)\n",
    "                \n",
    "    dummified = pd.get_dummies(df[to_dummify], dummy_na=True)\n",
    "    dummified = pd.concat([dummified, df], axis=1)\n",
    "    dummified_df = dummified.drop(to_dummify, axis=1)\n",
    "    \n",
    "    return to_dummify, dummified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumd_vars, df = dummify(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the remaining, not dummified categorical variables.\n",
    "#Let's convert them to ints (otherwise sklearn's random forest is grumpy)\n",
    "\n",
    "remaining_cat_vars = (list(set(cat_vars) - set(dumd_vars))) \n",
    "\n",
    "for var in remaining_cat_vars:\n",
    "    df[var] = df[var].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dependent variable (log-transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dependent variable (log)\n",
    "\n",
    "df['totals.transactionRevenue'] = pd.to_numeric(df['totals.transactionRevenue'], errors='coerce')\n",
    "df['totals.transactionRevenue'] = df['totals.transactionRevenue'].fillna(0)\n",
    "df[\"totals.transactionRevenue\"] = np.log1p(df[\"totals.transactionRevenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.581669    892138\n",
       "7.123673       256\n",
       "7.285507       189\n",
       "7.903227       187\n",
       "8.118207       170\n",
       "6.598509       135\n",
       "8.263591       122\n",
       "7.368970       116\n",
       "6.999423        98\n",
       "6.896694        93\n",
       "7.302496        92\n",
       "5.525453        84\n",
       "8.306719        81\n",
       "7.633853        77\n",
       "8.473032        65\n",
       "7.719574        64\n",
       "7.717796        62\n",
       "7.913887        54\n",
       "8.040125        51\n",
       "8.370085        46\n",
       "7.356918        44\n",
       "7.492203        40\n",
       "7.771067        40\n",
       "7.822044        39\n",
       "7.958577        39\n",
       "7.171657        38\n",
       "6.487684        37\n",
       "7.821242        37\n",
       "7.743703        37\n",
       "7.864036        37\n",
       "             ...  \n",
       "6.900731         1\n",
       "6.901737         1\n",
       "6.902743         1\n",
       "6.904751         1\n",
       "6.890609         1\n",
       "6.905753         1\n",
       "6.906755         1\n",
       "6.907755         1\n",
       "6.908755         1\n",
       "6.909753         1\n",
       "6.910751         1\n",
       "6.891626         1\n",
       "6.889591         1\n",
       "6.873164         1\n",
       "6.880384         1\n",
       "6.874198         1\n",
       "6.875232         1\n",
       "6.876265         1\n",
       "6.877296         1\n",
       "6.878326         1\n",
       "6.879356         1\n",
       "6.881411         1\n",
       "6.888573         1\n",
       "6.882438         1\n",
       "6.883462         1\n",
       "6.884487         1\n",
       "6.885509         1\n",
       "6.886532         1\n",
       "6.887553         1\n",
       "8.000014         1\n",
       "Name: totals.transactionRevenue, Length: 5333, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['totals.transactionRevenue'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf(f'{PATH}'+\"preprocessed_df.hdf\", key=\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
